# === Full clean.py with Fix for 'An√°lise' Column and Macrotheme Processing ===

import pandas as pd
import re
from pathlib import Path
import traceback
import os
from utils.plot_utils import generate_biweekly_dual_axis_chart

# === Constants ===
UNNECESSARY_COLUMNS = [
    "Descri√ß√£o monitoramento", "Link servi√ßo", "Descri√ß√£o Pai", "Link ocorr√™ncia Pai", "Thumbnail",
    "Thumbnail pai", "Data coleta", "Linguagem", "Foto publicador", "PageRank", "Estrelas",
    "Qualifica√ß√£o", "Qualificada por", "Data da qualifica√ß√£o", "Qualifica√ß√£o autom√°tica", "Para",
    "Latitude", "Longitude", "Id ocorr√™ncia no servi√ßo", "Id ocorr√™ncia pai no servi√ßo",
    "Link id ocorr√™ncia pai", "Arquivada", "Desarquivada", "Data Resposta",
    "Manifesta√ß√µes Detalhadas", "Termos", "Links", "Perfis", "Hashtags",
    "comments", "shares", "likes", "dislikes", "love", "wow", "haha", "sad", "angry",
    "thankful", "pride", "retweets", "favorites", "rating", "vendas", "resenhas", "votes", "views", "quotes",
    "videoViews", "URL da busca", "Id publicador", "Qualifica√ß√£o aprovada por", "Analisada por",
    "Data analisada", "Avalia√ß√£o", "Tipo/Conte√∫do", "Observa√ß√£o", "Unnamed: 73"
]

ENGAGEMENT_COLS = [
    "comments", "shares", "likes", "dislikes", "love", "wow", "haha", "sad", "angry",
    "thankful", "pride", "retweets", "favorites", "rating", "vendas",
    "resenhas", "votes", "views", "quotes"
]

LIST_CASA = ["C√ÇMARA", "SENADO"]
LIST_PARTIDO = ["MDB", "PT", "PRD", "PP", "PSDB", "PDT", "UNI√ÉO", "PL", "PODEMOS", "PSB", "REPUBLICANOS",
                "PV", "AVANTE", "PSC", "PSOL", "PCDOB", "PSD", "SOLIDARIEDADE", "NOVO", "REDE", "PMB",
                "UP", "DC", "PCO", "PSTU", "PCB", "PRTB", "MOBILIZA", "AGIR", "CIDADANIA", "PROS", "PATRIOTA"]
LIST_ESTADO = ["AC", "AL", "AP", "AM", "BA", "CE", "DF", "ES", "GO", "MA", "MT", "MS", "MG", "PA", "PB",
                "PR", "PE", "PI", "RJ", "RN", "RS", "RO", "RR", "SC", "SP", "SE", "TO"]

SPECIFIC_OVERRIDES = {
    "Nilto Tatto": {"Partido": "PT", "Estado": "SP"},
    "Socorro Neri": {"Partido": "PP", "Estado": "AC"},
    "AJ Albuquerque": {"Partido": "PP", "Estado": "CE"},
    "Duarte Junior": {"Partido": "PSB", "Estado": "MA"},
    "Julio Cesar": {"Partido": "PSD", "Estado": "PI"},
    "J√∫lio C√©sar": {"Partido": "PSD", "Estado": "PI"},
    "Vicentinho J√∫nior": {"Partido": "PP", "Estado": "TO"},
    "Yury do Pared√£o": {"Partido": "MDB", "Estado": "CE"}
}

# === Helper Functions ===

def clean_description(text):
    return re.sub(r'[\|:\*"<>\$\-\'%]', '', str(text))

def export_iramuteq(df, output_path):
    lines = []
    for _, row in df.iterrows():
        id_val = row.get("ID", "")
        nome = row.get("Nome publicador", "")
        descricao = clean_description(row.get("Descri√ß√£o", ""))
        lines.append(f"**** *id_{id_val} *u_{nome}\n{descricao}\n")

    with open(output_path, "w", encoding="utf-8") as f:
        f.writelines(lines)

def load_and_clean_sheet(filepath, sheet_name, is_main=True):
    df = pd.read_excel(filepath, sheet_name=sheet_name, skiprows=4)
    df.columns = df.columns.str.replace('"', '').str.strip()
    if is_main:
        df.insert(0, 'ID', range(1, len(df) + 1))
    return df

def clean_columns_and_values(df):
    engagement_present = [col for col in ENGAGEMENT_COLS if col in df.columns]
    if engagement_present:
        df["Manifesta√ß√µes reais"] = df[engagement_present].apply(pd.to_numeric, errors='coerce').sum(axis=1)
        if "Manifesta√ß√µes" in df.columns:
            idx = df.columns.get_loc("Manifesta√ß√µes") + 1
            cols = df.columns.tolist()
            cols.insert(idx, cols.pop(cols.index("Manifesta√ß√µes reais")))
            df = df[cols]

    df = df.drop(columns=set(UNNECESSARY_COLUMNS).intersection(df.columns), errors="ignore")
    df = df.replace("-", "NA").dropna(axis=1, how="all")

    for col in ['Perfil/Nome da busca', 'Servi√ßo']:
        if col in df.columns:
            df[col] = df[col].str.split(r"[-,]").str[0].str.strip()

    return df

def process_grupos_column(df):
    if "Grupos" in df.columns:
        df['Grupos'] = df['Grupos'].str.upper()

        def split_grupos(row):
            items = row.split(" | ") if isinstance(row, str) else []
            casa = partido = estado = extras = None
            items = ["C√ÇMARA" if i == "CAMARA" else i for i in items]
            items = ["PODEMOS" if i == "PODE" else i for i in items]
            for item in items:
                if item in LIST_CASA and not casa:
                    casa = item
                elif item in LIST_PARTIDO and not partido:
                    partido = item
                elif item in LIST_ESTADO and not estado:
                    estado = item
                else:
                    extras = extras + " | " + item if extras else item
            return casa, partido, estado, extras

        df[['Casa', 'Partido', 'Estado', 'Extras']] = df['Grupos'].apply(split_grupos).apply(pd.Series)
        df = df.drop(columns=['Grupos'])

    if 'Perfil/Nome da busca' in df.columns:
        for name, overrides in SPECIFIC_OVERRIDES.items():
            mask = df['Perfil/Nome da busca'].str.contains(name, na=False, case=False)
            for key, value in overrides.items():
                df.loc[mask, key] = value

    for col in ['Casa', 'Partido', 'Estado']:
        if col in df.columns and df[col].isnull().all():
            df = df.drop(columns=[col])

    return df

def enrich_parlamentar_and_date(df):
    def get_title(casa):
        return "Deputado(a)" if casa == "C√ÇMARA" else "Senador(a)" if casa == "SENADO" else ""

    if {'Casa', 'Perfil/Nome da busca', 'Partido', 'Estado'}.issubset(df.columns):
        df['Parlamentar'] = df.apply(
            lambda row: f"{get_title(row['Casa'])} {row['Perfil/Nome da busca']} ({row['Partido']}/{row['Estado']})",
            axis=1
        )

    if "Data publica√ß√£o" in df.columns:
        df["Data publica√ß√£o - Date"] = df["Data publica√ß√£o"].str[:8].str.strip()
        df["Data publica√ß√£o - Hour"] = df["Data publica√ß√£o"].str[9:].str.strip()
        df = df.drop(columns=["Data publica√ß√£o"])

    if "Data publica√ß√£o - Date" in df.columns:
        df["Data publica√ß√£o - Date"] = pd.to_datetime(df["Data publica√ß√£o - Date"], format="%d/%m/%y", errors='coerce')
        df["Data publica√ß√£o - Date"] = df["Data publica√ß√£o - Date"].dt.strftime("%d/%m/%Y")
        df["Dia"] = pd.to_datetime(df["Data publica√ß√£o - Date"], format="%d/%m/%Y", errors='coerce').dt.day
        df["M√™s"] = pd.to_datetime(df["Data publica√ß√£o - Date"], format="%d/%m/%Y", errors='coerce').dt.month
        df["Ano"] = pd.to_datetime(df["Data publica√ß√£o - Date"], format="%d/%m/%Y", errors='coerce').dt.year

    if "Data publica√ß√£o - Hour" in df.columns:
        df["Hora"] = df["Data publica√ß√£o - Hour"].str[:2]

    return df

def add_analysis_column(df):
    for col in ['ID', 'Descri√ß√£o', 'Manifesta√ß√µes', 'Link ocorr√™ncia']:
        if col not in df.columns:
            df[col] = ""
    df["An√°lise"] = (
        "ID: " + df["ID"].astype(str) +
        " | Texto: " + df["Descri√ß√£o"].astype(str) +
        " | Engajamento: " + df["Manifesta√ß√µes"].astype(str) +
        " | Link: " + df["Link ocorr√™ncia"].astype(str)
    )
    return df

# === Full Pipeline to Process, Export, and Save ===

def get_macrotheme_names(macrotheme_definitions):
    macrotheme_names = {}
    for macro, tags in macrotheme_definitions.items():
        name = " + ".join(tags)
        macrotheme_names[macro] = name
    return macrotheme_names

def assign_macrothemes(df, macrotheme_definitions):
    tag_columns = [col for col in df.columns if col not in df.columns[:df.columns.get_loc('Servi√ßo')+1]]
    assignments = pd.DataFrame(0, index=df.index, columns=['Macrotema'])
    for macro, tags in macrotheme_definitions.items():
        mask = df[tags].sum(axis=1) > 0
        assignments.loc[mask, 'Macrotema'] = macro
    return assignments

def export_macrotheme_txts(df, assignments, macrotheme_definitions, base_name, output_dir):
    """
    Gera arquivos .txt de macrotemas:
      ‚Ä¢ Usa as linhas de 'An√°lise' de cada macrotema.
      ‚Ä¢ Nomeia como: {base_name}_macrotema-{n}_{tags ou sem_tags}.txt
    """
    output_files = []
    # assignments: DataFrame com coluna 'Macrotema' (n√∫mero do tema)
    for macro, tags in macrotheme_definitions.items():
        # 1) Filtra as linhas atribu√≠das a este macrotema
        subset = df[assignments['Macrotema'] == macro]
        if subset.empty:
            continue

        # 2) Prepara sufixo de tags (underscore, min√∫sculas) ou 'sem_tags'
        name_part = "_".join(
            tag.lower().replace(" ", "_") for tag in tags
        ) or "sem_tags"

        # 3) Monta o caminho final
        file_path = output_dir / f"{base_name}_macrotema-{macro}_{name_part}.txt"

        # 4) Escreve cada linha de An√°lise no arquivo
        with open(file_path, "w", encoding="utf-8") as f:
            for line in subset["An√°lise"]:
                f.write(line + "\n")

        output_files.append(file_path)

    return output_files


def create_pivot_summary(df, assignments, macrotheme_definitions):
    df['Macrotema'] = assignments['Macrotema']

    # Total number of posts in the database
    total_posts = df.shape[0]

    # Get names for macrothemes
    macrotheme_names = get_macrotheme_names(macrotheme_definitions)

    # Map numbers to names
    df['Macrotema Nome'] = df['Macrotema'].map(macrotheme_names)
    df['Macrotema Nome'] = df['Macrotema Nome'].fillna('Sem Macrotema')

    # Create pivot
    summary = df.groupby(['Ano', 'M√™s', 'Dia', 'Macrotema Nome']).agg(
        Total_Publica√ß√µes=('ID', 'count'),
        Total_Engajamento=('Manifesta√ß√µes reais', 'sum')
    ).reset_index()

    # Sort
    summary = summary.sort_values(['M√™s', 'Dia'])

    return summary, total_posts

def create_microtheme_percentage(df, assignments):
    df['Macrotema'] = assignments['Macrotema']
    themes_cols = [col for col in df.columns if col not in df.columns[:df.columns.get_loc('Servi√ßo')+1]]

    numeric_df = df[themes_cols].apply(pd.to_numeric, errors='coerce')

    percentages = numeric_df.groupby(df['Macrotema']).mean() * 100
    percentages = percentages.round(2)
    return percentages

def create_relative_frequency_summary(df, assignments, macrotheme_definitions, total_posts):
    df['Macrotema'] = assignments['Macrotema']

    macrotheme_names = get_macrotheme_names(macrotheme_definitions)

    df['Macrotema Nome'] = df['Macrotema'].map(macrotheme_names)
    df['Macrotema Nome'] = df['Macrotema Nome'].fillna('Sem Macrotema')

    # Macrotheme Relative Frequency
    macro_counts = df['Macrotema Nome'].value_counts(normalize=True) * 100
    macro_freq = macro_counts.round(2).reset_index()
    macro_freq.columns = ['Macrotema', 'Frequ√™ncia Relativa (%)']

    # Microtheme Relative Frequency
    themes_cols = df.columns[df.columns.get_loc('Servi√ßo') + 1:]
    valid_theme_cols = [col for col in themes_cols if set(df[col].dropna().unique()).issubset({0, 1})]
    numeric_df = df[valid_theme_cols].apply(pd.to_numeric, errors='coerce')
    microtheme_freq = numeric_df.sum() / total_posts
    microtheme_freq = microtheme_freq.round(4).reset_index()
    microtheme_freq.columns = ['Microtema', 'Frequ√™ncia Relativa']

    return macro_freq, microtheme_freq

def full_pipeline(raw_filepath, macrotheme_definitions, cleaned_output_filename, width, height):
    raw_path = Path(raw_filepath)
    cleaned_path = raw_path.parent / cleaned_output_filename

    df_main = load_and_clean_sheet(raw_path, sheet_name="Ocorr√™ncias", is_main=True)

    try:
        df_tags = load_and_clean_sheet(raw_path, sheet_name="Tags", is_main=False)
        tag_fallback = pd.DataFrame(0, index=range(len(df_main)), columns=df_tags.columns)
        df_tags = df_tags.applymap(lambda x: 1 if str(x).strip().upper() == "SIM" else 0)
        rows_to_fill = min(len(df_tags), len(tag_fallback))
        tag_fallback.iloc[:rows_to_fill] = df_tags.iloc[:rows_to_fill]
        df_combined = pd.concat([df_main, tag_fallback], axis=1)
    except Exception:
        traceback.print_exc()
        df_combined = df_main.copy()

    df = clean_columns_and_values(df_combined)
    df = process_grupos_column(df)
    df = enrich_parlamentar_and_date(df)
    df = add_analysis_column(df)

    assignments = assign_macrothemes(df, macrotheme_definitions)

    # Export final Excel with pivots
    with pd.ExcelWriter(cleaned_path, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name='Cleaned Data')
        pivot_summary, total_posts = create_pivot_summary(df, assignments, macrotheme_definitions)
        pivot_summary.to_excel(writer, index=False, sheet_name='pvt_summary')
        macro_freq, microtheme_freq = create_relative_frequency_summary(
            df, assignments, macrotheme_definitions, total_posts
        )
        macro_freq.to_excel(writer, index=False, sheet_name='macro_freq')
        microtheme_freq.to_excel(writer, index=False, sheet_name='microtheme_freq')

    # ‚Äî‚Äî‚Äî Gera o gr√°fico quinzenal transparente ‚Äî‚Äî‚Äî (2025-7-3)
    # usa o pivot_summary j√° criado acima
    output_dir = cleaned_path.parent
    os.makedirs(output_dir, exist_ok=True)

    png_biweekly = output_dir / f"{clean_base}_vol.png"
    # pivot_summary vem do create_pivot_summary
    generate_biweekly_dual_axis_chart(
        pivot_summary,
        str(png_biweekly),
        width,
        height
    )
    print(f"üñºÔ∏è Gr√°fico quinzenal salvo como: {png_biweekly}")

    # --- Novo c√≥digo para definir o prefixo base sem "_cleaned" ---
    stem = cleaned_path.stem
    if stem.endswith("_cleaned"):
        clean_base = stem[:-len("_cleaned")]
    else:
        clean_base = stem

    # Prefixo para arquivos de macrotema com sufixo "_ai"
    ai_macro_base = f"{clean_base}_ai"
    # --- Fim do novo c√≥digo ---

    # Export macrotheme .txt usando o novo base_name (inclui "_ai")
    output_txts = export_macrotheme_txts(
        df,
        assignments,
        macrotheme_definitions,
        base_name=ai_macro_base,           # <‚Äî aqui usamos {base}_ai
        output_dir=cleaned_path.parent
    )

    # Export Iramuteq .txt com sufixo "_corpus.txt"
    iramuteq_txt_path = cleaned_path.parent / f"{clean_base}_corpus.txt"
    export_iramuteq(df, iramuteq_txt_path)

    return cleaned_path, output_txts, iramuteq_txt_path, png_biweekly